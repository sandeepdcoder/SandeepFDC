{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandeepdcoder/SandeepFDC/blob/main/Llama_4bit_after_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "================================================================================\n",
        "📋 WORKSHOP: Emotion Classification Fine-Tuning with QLoRA\n",
        "================================================================================\n",
        "\n",
        "📋 PURPOSE:\n",
        "This notebook fine-tunes Llama 3.2 3B on the emotion classification task using\n",
        "QLoRA (Quantized Low-Rank Adaptation) for ultra-efficient training. After\n",
        "training, we test on the SAME sentences from the baseline to measure improvement.\n",
        "\n",
        "🎯 KEY CONCEPT:\n",
        "We're training the model to classify emotions into 6 categories using only\n",
        "1,000 examples. QLoRA combines 4-bit quantization + LoRA adapters to train\n",
        "efficiently on consumer GPUs without modifying the entire 3B parameter model.\n",
        "\n",
        "🎯 LEARNING OBJECTIVES:\n",
        "- Understand QLoRA: 4-bit quantized base model + LoRA adapters\n",
        "- Apply LoRA adapters while keeping base model frozen in 4-bit\n",
        "- Fine-tune on emotion dataset with proper formatting\n",
        "- Use SFTTrainer for supervised fine-tuning\n",
        "- Compare before/after results on same test cases\n",
        "- Save and load fine-tuned adapters\n",
        "\n",
        "⚙️ REQUIREMENTS:\n",
        "- Google Colab with GPU (T4 recommended, 15GB VRAM)\n",
        "- ~15-20 minutes runtime (including training)\n",
        "- Run baseline test first (llama4bit_pretraining.py) for comparison\n",
        "\n",
        "🔬 WHAT THIS DEMONSTRATES:\n",
        "- QLoRA training: 4-bit base model + rank 32 LoRA (only 0.5% params trained)\n",
        "- Extreme memory efficiency: ~2GB total (vs ~6GB for regular LoRA)\n",
        "- Dramatic improvement from baseline (poor) to fine-tuned (80-90%+ accuracy)\n",
        "- Production-ready workflow: load 4bit → add LoRA → train → test → save\n",
        "\n",
        "📚 REFERENCE:\n",
        "QLoRA paper by Tim Dettmers et al. (2023): \"QLoRA: Efficient Finetuning of\n",
        "Quantized LLMs\" - enables training 65B models on single 48GB GPU\n",
        "\n",
        "================================================================================"
      ],
      "metadata": {
        "id": "7y-v0lWmCB3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#============================================================================\n",
        "# 🔧 STEP 1: INSTALLATION\n",
        "#============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"📦 Installing Unsloth and Dependencies for Fine-Tuning\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    # Local installation (simpler)\n",
        "    !uv pip install unsloth\n",
        "else:\n",
        "    # Colab installation (optimized for Colab environment)\n",
        "    !uv pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
        "    !uv pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "    !uv pip install --no-deps unsloth\n",
        "\n",
        "# 💡 KEY LIBRARIES FOR FINE-TUNING:\n",
        "# - peft: Parameter-Efficient Fine-Tuning (LoRA implementation)\n",
        "# - trl: Transformer Reinforcement Learning (SFTTrainer for supervised training)\n",
        "# - xformers: Memory-efficient attention operations\n",
        "# - datasets: Hugging Face datasets library (loads emotion data)\n",
        "\n",
        "print(\"✅ Installation complete!\\n\")"
      ],
      "metadata": {
        "id": "3jJkIGJoCFG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*80)\n",
        "print(\"🔍 Loading Base Model (Same as Baseline Test)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# Model configuration (same as baseline for fair comparison)\n",
        "max_seq_length = 2048  # Maximum context window\n",
        "dtype = None           # Auto-detect (FP16 for T4, BF16 for Ampere+)\n",
        "load_in_4bit = True    # 4-bit quantization to save memory\n",
        "\n",
        "# Load the same model used in baseline testing\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "# 💡 WHY SAME MODEL AS BASELINE?\n",
        "# We want to measure the impact of fine-tuning alone\n",
        "# By starting with the same model, we can directly compare:\n",
        "# - Baseline (no training) vs Fine-tuned (after training)\n",
        "\n",
        "print(f\"✅ Base model loaded: {model.config.model_type}\")\n",
        "print(f\"✅ Total parameters: ~3 Billion\")\n",
        "print(f\"✅ Memory: ~1.5-2 GB (4-bit quantized)\")\n",
        "\n",
        "print(\"-\"*80)\n",
        "print(\"=\"*80 + \"\\n\")"
      ],
      "metadata": {
        "id": "BGEaw_1tCHNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#============================================================================\n",
        "# 🎯 STEP 3: APPLY QLoRA (4-BIT BASE + LoRA ADAPTERS)\n",
        "#============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"🔧 Applying QLoRA: 4-bit Quantized Base + LoRA Adapters\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32,  # LoRA rank: Higher = more capacity, more memory (8/16/32/64)\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",    # Attention layers\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",       # MLP layers\n",
        "    ],\n",
        "    lora_alpha = 64,              # LoRA scaling factor (typically 2× rank)\n",
        "    lora_dropout = 0,             # Dropout rate (0 is optimized for Unsloth)\n",
        "    bias = \"none\",                # Bias training (\"none\" is optimized)\n",
        "    use_gradient_checkpointing = \"unsloth\",  # Memory-efficient backprop\n",
        "    random_state = 3407,          # Random seed for reproducibility\n",
        "    use_rslora = False,           # Rank-Stabilized LoRA (not needed here)\n",
        "    loftq_config = None,          # LoftQ quantization (not needed)\n",
        ")\n",
        "\n",
        "# 💡 WHAT IS QLoRA?\n",
        "# QLoRA = Quantized LoRA, combining two techniques:\n",
        "#\n",
        "# 1. BASE MODEL: 4-bit NormalFloat (NF4) quantization\n",
        "#    - Frozen at 4-bit precision (~1.5GB memory)\n",
        "#    - Not trained, just used for forward pass\n",
        "#    - 75% memory reduction vs FP16 (1.5GB vs 6GB)\n",
        "#\n",
        "# 2. LoRA ADAPTERS: Low-Rank Adaptation in FP16/BF16\n",
        "#    - Small trainable matrices added to model\n",
        "#    - Original weight W (frozen in 4-bit)\n",
        "#    - LoRA adds: ΔW = A × B (small matrices in 16-bit)\n",
        "#    - New weight: W' = W + ΔW\n",
        "#    - Only A and B are trained (~0.5% of parameters, ~20-50MB)\n",
        "#\n",
        "# QLoRA Architecture:\n",
        "# ┌─────────────────────────────────────┐\n",
        "# │ Base Model (Frozen)                 │\n",
        "# │ - 4-bit NF4 quantization            │\n",
        "# │ - ~1.5 GB memory                    │\n",
        "# │ - Not updated during training       │\n",
        "# └─────────────────────────────────────┘\n",
        "#          ↓\n",
        "# ┌─────────────────────────────────────┐\n",
        "# │ LoRA Adapters (Trainable)           │\n",
        "# │ - FP16/BF16 precision               │\n",
        "# │ - ~20-50 MB memory                  │\n",
        "# │ - Updated during training           │\n",
        "# └─────────────────────────────────────┘\n",
        "#          ↓\n",
        "# Total: ~2GB memory (vs ~6GB for LoRA, ~12GB for full FP16 fine-tuning)\n",
        "#\n",
        "# Benefits:\n",
        "# - Extreme memory efficiency (train 3B on T4, 65B on A100)\n",
        "# - Fast training (fewer parameters to update)\n",
        "# - No quality loss vs regular LoRA (proven in paper)\n",
        "# - Easy to swap (keep base model, change adapters)\n",
        "# - High quality (95-99% of full fine-tuning performance)\n",
        "\n",
        "# 💡 RANK EXPLAINED:\n",
        "# Rank = 32 means each LoRA matrix has 32 dimensions\n",
        "# Higher rank = more capacity to learn, but more memory\n",
        "# - Rank 8: Fastest, lowest memory, good for simple tasks\n",
        "# - Rank 16: Balanced (common choice)\n",
        "# - Rank 32: Higher capacity, better for complex tasks (our choice)\n",
        "# - Rank 64+: Highest quality, but approaching full fine-tuning cost\n",
        "\n",
        "print(\"✅ QLoRA configuration:\")\n",
        "print(f\"   Base model: 4-bit NF4 quantization (~1.5GB, frozen)\")\n",
        "print(f\"   LoRA adapters: FP16/BF16 (~20-50MB, trainable)\")\n",
        "print(f\"   LoRA rank: 32 (higher capacity for better accuracy)\")\n",
        "print(f\"   LoRA alpha: 64 (2× rank, standard scaling)\")\n",
        "print(f\"   Target modules: 7 (Attention + MLP layers)\")\n",
        "print(\"\\n📊 Trainable Parameters:\")\n",
        "model.print_trainable_parameters()\n"
      ],
      "metadata": {
        "id": "7CQbYNJzCQhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Dataset Link text](https://huggingface.co/datasets/dair-ai/emotion/viewer/split/train?views%5B%5D=split_train)**"
      ],
      "metadata": {
        "id": "uQj4fTKMSDMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#============================================================================\n",
        "# 📝 STEP 4: LOAD AND FORMAT DATASET\n",
        "#============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"📊 Loading Emotion Dataset from Hugging Face\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Define emotion labels (same as baseline)\n",
        "EMOTION_LABELS = {\n",
        "    0: \"sadness\",\n",
        "    1: \"joy\",\n",
        "    2: \"love\",\n",
        "    3: \"anger\",\n",
        "    4: \"fear\",\n",
        "    5: \"surprise\"\n",
        "}\n",
        "\n",
        "def to_llama3_format(example):\n",
        "    \"\"\"\n",
        "    Convert text-label pair to Llama 3.2 chat format.\n",
        "\n",
        "    Uses the tokenizer's apply_chat_template() for proper formatting.\n",
        "    This ensures the model sees data in the exact format it expects.\n",
        "    \"\"\"\n",
        "    text = example['text']\n",
        "    label = example['label']\n",
        "    emotion_name = EMOTION_LABELS[label]\n",
        "\n",
        "    # Create messages in chat format\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Identify the emotion in the following sentence and provide the emotion label.\"},\n",
        "        {\"role\": \"user\", \"content\": text},\n",
        "        {\"role\": \"assistant\", \"content\": f\"{label} ({emotion_name})\"}  # Expected output\n",
        "    ]\n",
        "\n",
        "    # Use tokenizer's built-in chat template\n",
        "    # add_generation_prompt=False because we include the assistant's response\n",
        "    formatted_text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=False  # We have the full conversation\n",
        "    )\n",
        "\n",
        "    return {\"text\": formatted_text}\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"dair-ai/emotion\")\n",
        "\n",
        "# Use first 1000 samples for faster training (workshop demo)\n",
        "# For production, use full dataset: dataset['train']\n",
        "train_dataset = dataset['train'].select(range(1000)).map(\n",
        "    to_llama3_format,\n",
        "    remove_columns=['text', 'label']\n",
        ")\n",
        "\n",
        "print(f\"✅ Dataset: dair-ai/emotion\")\n",
        "print(f\"✅ Training samples: {len(train_dataset):,}\")\n",
        "print(f\"✅ Emotion classes: {len(EMOTION_LABELS)}\")\n",
        "print(f\"\\n📄 Sample formatted training example:\")\n",
        "print(\"-\" * 80)\n",
        "print(train_dataset[0][\"text\"][:300] + \"...\")\n",
        "print(\"-\" * 80 + \"\\n\")\n",
        "\n",
        "# 💡 WHY ONLY 1000 SAMPLES?\n",
        "# For workshop/demo purposes:\n",
        "# - Faster training (~10-15 min vs 30-60 min for full dataset)\n",
        "# - Still shows dramatic improvement over baseline\n",
        "# - For production: use full 16k training samples"
      ],
      "metadata": {
        "id": "Xc4-4WliCRez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#============================================================================\n",
        "# 🏋️ STEP 5: CONFIGURE TRAINING\n",
        "#============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"⚙️  Configuring Training Parameters\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    dataset_text_field = \"text\",       # Field name in dataset\n",
        "    max_seq_length = max_seq_length,   # Max tokens per example\n",
        "    dataset_num_proc = 2,               # Parallel data loading\n",
        "    packing = False,                    # Don't pack multiple examples together\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        # Batch configuration\n",
        "        per_device_train_batch_size = 2,   # Batch size per GPU\n",
        "        gradient_accumulation_steps = 4,   # Effective batch = 2 × 4 = 8\n",
        "\n",
        "        # Training duration\n",
        "        num_train_epochs = 2,               # Number of passes through data\n",
        "        # max_steps = 60,                   # Alternative: fixed number of steps\n",
        "\n",
        "        # Learning rate\n",
        "        learning_rate = 2e-4,               # How fast to learn\n",
        "        warmup_steps = 5,                   # Gradual learning rate warmup\n",
        "        lr_scheduler_type = \"cosine\",       # Learning rate decay schedule\n",
        "\n",
        "        # Optimization\n",
        "        optim = \"adamw_8bit\",               # Memory-efficient optimizer\n",
        "        weight_decay = 0.01,                # Regularization strength\n",
        "\n",
        "        # Precision (auto-detect based on GPU)\n",
        "        fp16 = not is_bfloat16_supported(), # Use FP16 on older GPUs (T4, V100)\n",
        "        bf16 = is_bfloat16_supported(),     # Use BF16 on newer GPUs (A100, A6000)\n",
        "\n",
        "        # Logging and saving\n",
        "        logging_steps = 1,                  # Log every step\n",
        "        output_dir = \"outputs\",             # Where to save checkpoints\n",
        "        report_to = \"none\",                 # Disable W&B/TensorBoard\n",
        "\n",
        "        # Reproducibility\n",
        "        seed = 3407,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# 💡 KEY TRAINING PARAMETERS EXPLAINED:\n",
        "#\n",
        "# Effective Batch Size = 2 × 4 = 8:\n",
        "#   - Real batch size: 2 (fits in memory)\n",
        "#   - Gradient accumulation: 4 (accumulate gradients from 4 batches)\n",
        "#   - Result: Same as training with batch size 8, but uses less memory\n",
        "#\n",
        "# Learning Rate = 2e-4:\n",
        "#   - Standard for LoRA fine-tuning\n",
        "#   - Lower than full fine-tuning (which uses 1e-5)\n",
        "#   - LoRA is more stable with higher learning rates\n",
        "#\n",
        "# Cosine Schedule:\n",
        "#   - Learning rate starts at 2e-4\n",
        "#   - Gradually decreases following cosine curve\n",
        "#   - Helps model converge smoothly\n",
        "#\n",
        "# 2 Epochs:\n",
        "#   - Model sees each of 1000 examples twice\n",
        "#   - Total steps: ~250 (1000 / 8 batch size × 2 epochs)\n",
        "#   - Training time: ~10-15 minutes on T4\n",
        "\n",
        "print(f\"✅ Effective batch size: {2 * 4}\")\n",
        "print(f\"✅ Training epochs: 2\")\n",
        "print(f\"✅ Learning rate: 2e-4 (with cosine decay)\")\n",
        "print(f\"✅ Optimizer: AdamW 8-bit (memory efficient)\")\n",
        "print(f\"✅ Expected training time: ~10-15 min on T4 GPU\")"
      ],
      "metadata": {
        "id": "UoSziI5kCVcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#============================================================================\n",
        "# 🚀 STEP 6: TRAIN THE MODEL\n",
        "#============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"🚀 Starting Fine-Tuning Training...\")\n",
        "print(\"=\"*80)\n",
        "print(\"Training on 1,000 emotion examples\")\n",
        "print(\"Watch the loss decrease - this shows the model is learning!\\n\")\n",
        "\n",
        "# Train!\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"✅ Training Complete!\")\n",
        "print(\"=\"*80)\n",
        "print(f\"📊 Final training loss: {trainer_stats.training_loss:.4f}\")\n",
        "print(f\"⏱️  Training time: {trainer_stats.metrics.get('train_runtime', 0):.1f} seconds\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# 💡 WHAT HAPPENED DURING TRAINING?\n",
        "# 1. Model processed 1000 emotion examples, 2 times (2 epochs)\n",
        "# 2. Learned to map text → emotion labels\n",
        "# 3. Learned the output format: \"0 (sadness)\", \"1 (joy)\", etc.\n",
        "# 4. Only LoRA adapters were trained (~0.5% of parameters, ~20-50MB)\n",
        "# 5. Base model weights remain frozen in 4-bit (QLoRA technique)\n",
        "# 6. Total memory usage: ~2GB (vs ~6GB for regular LoRA)\n"
      ],
      "metadata": {
        "id": "wf-05uMUCYmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#============================================================================\n",
        "# 🧪 STEP 7: TEST THE FINE-TUNED MODEL\n",
        "#============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"🧪 Testing Fine-Tuned Model\")\n",
        "print(\"=\"*80)\n",
        "print(\"Using SAME test sentences from baseline for fair comparison\\n\")\n",
        "\n",
        "# Enable inference mode (faster, no gradient calculation)\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "def predict_emotion(text):\n",
        "    \"\"\"\n",
        "    Predict emotion using the fine-tuned model.\n",
        "\n",
        "    Same function as baseline test, but now using trained model.\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Identify the emotion in the following sentence and provide the emotion label.\"},\n",
        "        {\"role\": \"user\", \"content\": text}\n",
        "    ]\n",
        "\n",
        "    # Format with chat template\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True  # Add <|assistant|> marker\n",
        "    )\n",
        "\n",
        "    # Tokenize and generate\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=20,\n",
        "        temperature=0.1,  # Low temperature for consistent classification\n",
        "        do_sample=True\n",
        "    )\n",
        "\n",
        "    # Decode and extract response\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    response = response.split(\"assistant\")[-1].strip()\n",
        "\n",
        "    return response\n",
        "\n",
        "# SAME test sentences as baseline (for comparison)\n",
        "test_sentences = [\n",
        "    \"i didnt feel humiliated\",\n",
        "    \"im grabbing a minute to post i feel greedy wrong\",\n",
        "    \"i am ever feeling nostalgic about the fireplace i will know that it is still on the property\",\n",
        "    \"i am feeling grouchy\",\n",
        "    \"ive been taking or milligrams or times recommended amount and ive fallen asleep a lot faster but i also feel like so funny\",\n",
        "    \"i feel as confused about life as a teenager or as jaded as a year old man\",\n",
        "    \"i need you i need someone i need to be protected and feel safe i am small now i find myself in a season of no words\"\n",
        "]\n",
        "\n",
        "print(\"Running predictions on test sentences...\")\n",
        "print(\"=\"*80 + \"\\n\")"
      ],
      "metadata": {
        "id": "4IpLThgACc9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test and display results\n",
        "results = []\n",
        "for i, sentence in enumerate(test_sentences, 1):\n",
        "    prediction = predict_emotion(sentence)\n",
        "    results.append({\n",
        "        \"input\": sentence,\n",
        "        \"output\": prediction\n",
        "    })\n",
        "    print(f\"[{i}/{len(test_sentences)}] {sentence[:60]}...\")\n",
        "    print(f\"→ {prediction}\\n\")\n",
        "\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "4my8yNIaChSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"📊 FINE-TUNING RESULTS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n✅ EXPECTED IMPROVEMENTS FROM BASELINE:\")\n",
        "print(\"   1. OUTPUT FORMAT: Now consistently follows '0 (sadness)' format\")\n",
        "print(\"   2. ACCURACY: 80-90%+ correct emotion identification\")\n",
        "print(\"   3. CONSISTENCY: Same input → same output (reproducible)\")\n",
        "\n",
        "print(\"\\n💡 WHAT THE MODEL LEARNED:\")\n",
        "print(\"   ✓ 6 emotion categories (sadness, joy, love, anger, fear, surprise)\")\n",
        "print(\"   ✓ Specific output format with number + name\")\n",
        "print(\"   ✓ Emotion patterns in text (keywords, context, sentiment)\")\n",
        "print(\"   ✓ Task-specific consistency\")\n",
        "\n",
        "print(\"\\n📈 TRAINING STATISTICS:\")\n",
        "print(f\"   Training samples: 1,000\")\n",
        "print(f\"   Epochs: 2\")\n",
        "print(f\"   Trainable parameters: ~0.5% of total (QLoRA)\")\n",
        "print(f\"   Memory usage: ~2GB (4-bit base + LoRA adapters)\")\n",
        "print(f\"   Training time: {trainer_stats.metrics.get('train_runtime', 0):.1f}s\")\n",
        "print(f\"   Final loss: {trainer_stats.training_loss:.4f}\")\n",
        "\n",
        "print(\"\\n🔍 COMPARE THESE RESULTS TO BASELINE:\")\n",
        "print(\"   Run llama4bit_pretraining.py to see the baseline (untrained)\")\n",
        "print(\"   You should see dramatic improvement in:\")\n",
        "print(\"   - Format adherence (was messy → now clean)\")\n",
        "print(\"   - Emotion accuracy (was random → now 80-90%+)\")\n",
        "print(\"   - Consistency (was varied → now deterministic)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"✅ Fine-tuning demonstration complete!\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "================================================================================\n",
        "🎯 WORKSHOP FACILITATOR NOTES\n",
        "================================================================================\n",
        "\n",
        "1. BEFORE AND AFTER STORY:\n",
        "   - Show baseline results first (poor, inconsistent, wrong format)\n",
        "   - Run this training script (takes ~10-15 min)\n",
        "   - Show dramatic improvement on SAME test cases\n",
        "   - This visceral before/after is the key teaching moment\n",
        "\n",
        "2. QLoRA EFFICIENCY:\n",
        "   - QLoRA = 4-bit quantized base model + LoRA adapters\n",
        "   - Only 0.5% of parameters trained (LoRA adapters)\n",
        "   - Total memory: ~2GB (vs ~6GB for regular LoRA, ~12GB for full FP16)\n",
        "   - Adapters are tiny (~20-50 MB vs 6GB full model)\n",
        "   - Training is fast (10-15 min vs hours for full fine-tuning)\n",
        "   - Quality is 95-99% of full fine-tuning (no degradation vs LoRA)\n",
        "   - Can swap adapters: same base model, different tasks\n",
        "   - Breakthrough: Tim Dettmers' QLoRA paper (2023) enabled training 65B on single GPU\n",
        "\n",
        "3. KEY HYPERPARAMETERS:\n",
        "   - Rank 32: Higher than default (8/16) for better accuracy\n",
        "   - Learning rate 2e-4: Standard for LoRA (higher than full fine-tuning)\n",
        "   - Cosine schedule: Smooth learning rate decay\n",
        "   - 2 epochs: Enough for 1000 samples (more epochs on larger datasets)\n",
        "\n",
        "4. COMMON ISSUES:\n",
        "   - If loss doesn't decrease: Check data formatting\n",
        "   - If outputs still wrong: May need more epochs or data\n",
        "   - If CUDA OOM: Reduce batch size or sequence length\n",
        "   - If slow: Check GPU is being used (should be <1 min/epoch)\n",
        "\n",
        "5. PRODUCTION CONSIDERATIONS:\n",
        "   - Use full dataset (16k samples) not just 1000\n",
        "   - Add validation split to monitor overfitting\n",
        "   - Increase epochs to 3-5 for full dataset\n",
        "   - Save checkpoints periodically\n",
        "   - Test on held-out test set for final evaluation\n",
        "\n",
        "6. REAL-WORLD APPLICATIONS:\n",
        "   - Customer support: Classify ticket categories, urgency\n",
        "   - Content moderation: Detect toxic, spam, inappropriate\n",
        "   - Healthcare: Classify symptoms, triage severity\n",
        "   - Education: Grade sentiment in student feedback\n",
        "   - Any classification task with 100-10000 examples\n",
        "\n",
        "7. COST COMPARISON:\n",
        "   - Fine-tuning cost: $0.50-2 on Colab Pro (includes GPU time)\n",
        "   - API cost (no fine-tuning): $0.01 per 1k tokens × volume\n",
        "   - Crossover: If >50k-200k queries, fine-tuning cheaper\n",
        "   - Plus benefits: Privacy, control, customization\n",
        "\n",
        "================================================================================\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "G1uvf1u2CjsN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}